{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/home/richhiey/Desktop/workspace/academics/courses/semester_4/Advanced Topics in Machine Learning/course_project\"\n",
    "CORPUS_PATH = os.path.join(BASE_PATH, \"Gutenberg_English_Fiction_1k\")\n",
    "DATA_PATH = os.path.join(CORPUS_PATH, \"Gutenberg_19th_century_English_Fiction\")\n",
    "FEATURES_PATH = os.path.join(BASE_PATH, \"ATiML-Project\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Literary                       792\n",
       "Detective and Mystery          111\n",
       "Sea and Adventure               36\n",
       "Western Stories                 18\n",
       "Love and Romance                18\n",
       "Ghost and Horror                 6\n",
       "Humorous and Wit and Satire      6\n",
       "Christmas Stories                5\n",
       "Allegories                       2\n",
       "Name: guten_genre, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMN_NAMES = [\n",
    "    'Filename',\n",
    "    'Number of Named People',\n",
    "    'Number of Named Places',\n",
    "    'Number of Named Organizations',\n",
    "    'Number of Sentences',\n",
    "    'Female Orientation',\n",
    "    'Male Orientation',\n",
    "    'Positive Sentiment',\n",
    "    'Negative Sentiment',\n",
    "    'Objective Sentiment',\n",
    "    'Number of Words',\n",
    "    'Number of Paragraphs',\n",
    "    'Relative Punctuation',\n",
    "    'Number of Dialogs',\n",
    "    'Number of Sentences with Dialogs'  \n",
    "]\n",
    "all_features = pd.read_csv(os.path.join(FEATURES_PATH, \"all_features_raw.csv\"), usecols=COLUMN_NAMES)\n",
    "meta_data = pd.read_csv(os.path.join(CORPUS_PATH, \"master996.csv\"), sep=\";\", header=0, encoding='latin1')\n",
    "merged_data = pd.merge(all_features, meta_data, left_on=\"Filename\", right_on=\"book_id\")\n",
    "merged_data[\"guten_genre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and labels to be used for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem that we face with the dataset, even after extracting explainable features, is that the output classes are imbalanced. The number of labeled instances for each of the classes is shown below:\n",
    "\n",
    "Literary:**792**\n",
    "\n",
    "Detective and Mystery:**111**\n",
    "\n",
    "Sea and Adventure:**36**\n",
    "\n",
    "Western Stories:**18**\n",
    "\n",
    "Love and Romance:**18**\n",
    "\n",
    "Ghost and Horror:**6**\n",
    "\n",
    "Humorous and Wit and Satire:**6**\n",
    "\n",
    "Christmas Stories:**5**\n",
    "\n",
    "Allegories:**2**\n",
    "\n",
    "As we can see, the last 4 entries seem outnumbered compared to the first. The label \"Allegories\" specially has only 2 labeled instances corresponding to it. I don't know how useful it could be predict such a class. In my opinion, it would be better to skip this class and focus on the others.\n",
    "\n",
    "In the case of \"Christmas Stories, \"Humor..\", and \"Ghost and Horror\", we have counts of 6, 6 and 5 respectively. Hence, I allowed for the possibility of duplicates and upsampled to get 20 instances in each class. This is a simple hack which increases the signal strength for any learning algorithm for a particular class.\n",
    "\n",
    "Similarly, we round off \"Love and Romance\" and \"Western Stories\" to 40 instances and \"Sea and Adventure\" to 60 instances by upsampling.\n",
    "\n",
    "For the other two majority classes \"Literary\" and \"Detective and Mystery\", we do not upsample.\n",
    "\n",
    "Overall, by upsampling, the dataset now looks something like this- \n",
    "\n",
    "Literary:**792**\n",
    "\n",
    "Detective and Mystery:**111**\n",
    "\n",
    "Sea and Adventure:**60**\n",
    "\n",
    "Western Stories:**40**\n",
    "\n",
    "Love and Romance:**40**\n",
    "\n",
    "Ghost and Horror:**20**\n",
    "\n",
    "Humorous and Wit and Satire:**20**\n",
    "\n",
    "Christmas Stories:**20**\n",
    "\n",
    "I don't know whether this is the right way to solve a class imbalance, but it seemed like a reasonable approach to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "COLUMNS_TO_DROP = [\"Filename\", \"Book_Name\", \"book_id\", \"Author_Name\", \"guten_genre\"]\n",
    "\n",
    "# THIS METHOD IS A PROBLEM - Our classes are super imbalanced, so doing a random split might \n",
    "# lead to some classes not even being in the test or train set\n",
    "labels = merged_data[['guten_genre']]\n",
    "data = merged_data.drop(COLUMNS_TO_DROP, axis=1)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(min_max_scaler.fit_transform(data), labels, stratify=labels, test_size=0.2)\n",
    "\n",
    "# Custom splitting of classes is needed!\n",
    "def create_training_and_test_sets(labeled_instances, test_size, resample_num):\n",
    "    resampled = resample(\n",
    "        labeled_instances, \n",
    "        replace=True,\n",
    "        n_samples=resample_num,\n",
    "        random_state=123\n",
    "    )\n",
    "    return train_test_split(resampled, test_size=test_size)\n",
    "\n",
    "train_dataset = pd.DataFrame() \n",
    "test_dataset = pd.DataFrame()\n",
    "\n",
    "# Skipping label=Allegories since there are only two instances\n",
    "labels = [\n",
    "    \"Detective and Mystery\",\n",
    "    \"Literary\",\n",
    "    \"Christmas Stories\",\n",
    "    \"Western Stories\",\n",
    "    \"Sea and Adventure\",\n",
    "    \"Love and Romance\",\n",
    "    \"Humorous and Wit and Satire\",\n",
    "    \"Ghost and Horror\"\n",
    "]\n",
    "\n",
    "for label in labels:\n",
    "    sliced_dataset = merged_data[merged_data.guten_genre == label]\n",
    "    if (len(sliced_dataset) < 10):\n",
    "        # If count is very small, then upsample\n",
    "        train_set, test_set = create_training_and_test_sets(sliced_dataset, 0.3, 20)\n",
    "    elif (len(sliced_dataset) < 20):\n",
    "        train_set, test_set = create_training_and_test_sets(sliced_dataset, 0.3, 40)\n",
    "    elif (len(sliced_dataset) < 40):\n",
    "        # If count is less than 100, then resample to 50 \n",
    "        train_set, test_set = create_training_and_test_sets(sliced_dataset, 0.3, 60)\n",
    "    else:\n",
    "        # If count is very large, then split directly\n",
    "        train_set, test_set = train_test_split(sliced_dataset, test_size=0.3)\n",
    "    train_dataset = pd.concat([train_dataset, train_set])\n",
    "    test_dataset = pd.concat([test_dataset, test_set])\n",
    "\n",
    "\n",
    "x_train = train_dataset.drop(COLUMNS_TO_DROP, axis=1)\n",
    "x_test = test_dataset.drop(COLUMNS_TO_DROP, axis=1)\n",
    "x_train = pd.DataFrame(min_max_scaler.fit_transform(x_train))\n",
    "x_test = pd.DataFrame(min_max_scaler.fit_transform(x_test))\n",
    "y_train = train_dataset[[\"guten_genre\"]]\n",
    "y_test = test_dataset[[\"guten_genre\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Classification with Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7168674698795181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression(max_iter=100000).fit(x_train, y_train)\n",
    "print(lreg.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7168674698795181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb = nb.fit(x_train, y_train)\n",
    "print(nb.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Classification with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713855421686747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(kernel='linear',gamma='auto'))\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6987951807228916\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=10000).fit(x_train, y_train)\n",
    "print(nn.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
